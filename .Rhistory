tracts$Tract = as.character(tracts$Tract) # Convert tract to char
pittsburgh.census = pittsburgh.tract %>% # Filter for tracts that are in the HFPA Sheet
filter(GEOID %in% tracts$Tract)
plot(pittsburgh.census['estimate'])
ggplot(data = pittsburgh.census, aes(fill = estimate)) + # Plot only the PGH Tracts
geom_sf(color = 'black', size = 0.2) +
scale_fill_distiller(palette = 'Oranges') +
theme_void()
# Get No Vehicle Availability
vehicle.availability = get_acs(
geography = 'tract',
variables = 'B08014_002', # This is the num_households, no vehicle available variable
year = 2020, # Supplied Year
state = 'PA',
county = 'Allegheny',
geometry = TRUE
)
vehicle.availability = vehicle.availability %>%
pivot_wider(id_cols = c(GEOID, NAME, geometry), names_from = variable, values_from = estimate)
#rename(no.vehicle.avail = estimate) %>%
#filter(GEOID %in% tracts$Tract)
pgh.tracts = tracts$Tract # Turn tract df into a list
# Function that calls census API given variable, returns dataframe of GEOID(tract) and estimate (with named column)
get.census.var = function(var.id, year = 2020, var.name, geometry = FALSE) {
var.df = get_acs(
geography = 'tract',
variables = var.id, # This is the var.id from https://api.census.gov/data/2020/acs/acs5/variables.html
year = year, # Supplied Year
state = 'PA',
county = 'Allegheny',
geometry = geometry
)
var.df = var.df %>%
select(GEOID, estimate) %>%
filter(GEOID %in% pgh.tracts) %>%
rename(!!var.name := estimate) %>%
mutate(year = year) # Add column for year
return(var.df)
}
# Test the function with no vehicle available
vehicle.availability = get.census.var(var.id = 'B08014_002', var.name = 'no.vehicle.avail')
num.hh = get.census.var(var.id = 'B08201_001', var.name = 'num.hh')
# Use the original pgh census dataframe and add this variable to it
pittsburgh.census = pittsburgh.census %>%
inner_join(y = vehicle.availability, by = 'GEOID')
# join the num.hh
pittsburgh.census = pittsburgh.census %>%
inner_join(y = num.hh, by = 'GEOID')
# get no vehicle score
hh.no.vehicle = pittsburgh.census %>%
select(-c(variable, estimate, moe)) %>%
mutate(pct.no.vehicle = no.vehicle.avail / num.hh)
# Get current year
current.year = as.numeric(format(Sys.Date(), '%Y'))
last.10.years = seq(from = current.year - 2, to = current.year - 11, by = -1)
last.5.years = seq(from = current.year - 2, to = current.year - 6, by = -1)
test.df = data.frame(GEOID = c(), num.hh = c(), year = c())
for (year in last.5.years) {
year.df = get.census.var(var.id = 'B08201_001', var.name = 'num.hh', year = year)
test.df = rbind(test.df, year.df) # add years df to main df
}
filtered.tracts = c('42003151700', '42003141400')
ggplot(data = filter(test.df, GEOID %in% filtered.tracts),
mapping = aes(x = year, y = num.hh)) +
geom_line() +
geom_point(size = 3) +
facet_grid(rows = 'GEOID')
# Get Availability Data (using 2017 data for now)
# Create first dataset with GEOMETRY = TRUE (Total House Holds)
availability = get.census.var(var.id = 'B08201_001', year = 2017, var.name = 'Total_HH', geometry = TRUE)
# Get No_Vehicle_Avail and join to above dataset
availability = availability %>%
inner_join(y = select(get.census.var(var.id = 'B08014_002', year = 2017, var.name = 'No_Vehicle_Avail'), c(GEOID, No_Vehicle_Avail)), by = 'GEOID')
# Get %_no_veh_avail, no_veh_std, no_vechicle_score
availability = availability %>%
mutate(pct_no_veh_avail = Total_HH/No_Vehicle_Avail,
pct_no_veh_avail = replace(pct_no_veh_avail, is.nan(pct_no_veh_avail) | is.infinite(pct_no_veh_avail), NA), # Replace NaN and Inf with NA
no_veh_std = (pct_no_veh_avail - (sum(No_Vehicle_Avail, na.rm = T) / sum(Total_HH, na.rm = T))) / sd(pct_no_veh_avail, na.rm = T),
No_Vehicle_Score = (no_veh_std - min(no_veh_std, na.rm = T)) * (10 / (max(no_veh_std, na.rm = T) - min(no_veh_std, na.rm = T))))
# Import Walkability from HFPA Excel Sheet
no.walkshed = read_csv('Data/no_walkshed.csv')
no.walkshed = mutate(no.walkshed, GEOID = as.character(GEOID))
# Add pct_walkable, pct_walkable_std, Walkability_Score
availability = availability %>%
inner_join(y = no.walkshed, by = 'GEOID') %>% # Get no_walkshed
mutate(pct_walkable_std = (no_walkshed - mean(no_walkshed, na.rm = T)) / sd(no_walkshed, na.rm = T),
Walkability_Score = (pct_walkable_std - min(pct_walkable_std, na.rm = T)) * 10 / (max(pct_walkable_std, na.rm = T) - min(pct_walkable_std, na.rm = T)))
# Calculate Final Availability and Drop Unnecessary Columns
availability = availability %>%
mutate(Availability = sqrt(No_Vehicle_Score * Walkability_Score)) %>%
select(GEOID, No_Vehicle_Score, Walkability_Score, Availability, year)
plot(availability['Availability'])
# Make a normalized.score function that will take the main metric to be standardized and output standardized score on a 1-10 scale
normalized.score = function(unstandardized) {
standardized = (unstandardized - mean(unstandardized, na.rm = T)) / sd(unstandardized, na.rm = T)
normalized = (standardized - min(standardized, na.rm = T)) * (10 / (max(standardized, na.rm = T) - min(standardized, na.rm = T)))
return(normalized)
}
# Get Access Data (Using 2017 Data)
access = get.census.var(var.id = 'S1701_C01_041', year = 2017, var.name = 'pop_below_185', geometry = TRUE)
# Get pop below 185% Poverty Level
access = access %>%
inner_join(y = select(get.census.var(var.id = 'S0101_C01_001', year = 2017, var.name = 'total_pop'), c(GEOID, total_pop)), by = 'GEOID')
# Create the rest of the columns
access = access %>%
mutate(pct_below_185 = pop_below_185 / total_pop,
Access = normalized.score(pct_below_185))
plot(access['Access'])
# Re-Do Availability Using Mean and new Function
# Get Availability Data (using 2017 data for now)
# Create first dataset with GEOMETRY = TRUE (Total House Holds)
availability = get.census.var(var.id = 'B08201_001', year = 2017, var.name = 'Total_HH', geometry = TRUE)
# Get No_Vehicle_Avail and join to above dataset
availability = availability %>%
inner_join(y = select(get.census.var(var.id = 'B08014_002', year = 2017, var.name = 'No_Vehicle_Avail'), c(GEOID, No_Vehicle_Avail)), by = 'GEOID')
# Get %_no_veh_avail, no_veh_std, no_vechicle_score
availability = availability %>%
mutate(pct_no_veh_avail = Total_HH/No_Vehicle_Avail,
pct_no_veh_avail = replace(pct_no_veh_avail, is.nan(pct_no_veh_avail) | is.infinite(pct_no_veh_avail), NA), # Replace NaN and Inf with NA
No_Vehicle_Score = normalized.score(pct_no_veh_avail))
# Import Walkability from HFPA Excel Sheet
no.walkshed = read_csv('Data/no_walkshed.csv')
no.walkshed = mutate(no.walkshed, GEOID = as.character(GEOID))
# Add pct_walkable, pct_walkable_std, Walkability_Score
availability = availability %>%
inner_join(y = no.walkshed, by = 'GEOID') %>% # Get no_walkshed
mutate(Walkability_Score = normalized.score(no_walkshed))
# Calculate Final Availability and Drop Unnecessary Columns
availability = availability %>%
mutate(Availability = sqrt(No_Vehicle_Score * Walkability_Score))
plot(availability['Availability'])
# 500 Cities: Census Tract-level Data
# Create a way to get the closest year's data for example passing 2020 would get 2019 data, passing in 2015 would get 2016 data
# This is a hardcoded dataframe. To find new URL endpoints for different years go here:
# https://chronicdata.cdc.gov/browse?category=500+Cities+%26+Places&q=500+Cities%3A+Census+Tract-level+Data&sortBy=relevance
file.names = c('k86t-wghb.csv', 'k25u-mg9b.csv', 'kucs-wizg.csv', '5mtz-k78d.csv')
years = c(2019, 2018, 2017, 2016)
API.endpoints = data.frame(year = years, file.name = file.names)
get.500.cities.endpoint = function(supplied.year) {
closest.year = which(abs(API.endpoints$year - supplied.year) == min(abs(API.endpoints$year - supplied.year)))
endpoint = API.endpoints[closest.year, 'file.name']
return(endpoint)
}
# Get URL for 2017
year = 2017
URL = paste('https://chronicdata.cdc.gov/resource/', get.500.cities.endpoint(year), '?placename=Pittsburgh', sep='')
utilization = read_csv(URL)
utilization = utilization %>%
mutate(GEOID = as.character(tractfips), # change to GEOID
year = year) %>%  # Keep the year
select(GEOID, year, obesity_crudeprev, chd_crudeprev, diabetes_crudeprev) %>% # Keep only necessary columns
mutate(obesity_norm = normalized.score(obesity_crudeprev), # Normalize the columns
chd_norm = normalized.score(chd_crudeprev),
diabetes_norm = normalized.score(diabetes_crudeprev),
Utilization = obesity_norm + chd_norm + diabetes_norm) # Calculate utilization!
View(year.df)
?left_join
runApp('Desktop/Food Policy/HFPA_app')
# Create the rest of the columns
year.data = year.data %>%
mutate(pct_below_185 = pop_below_185 / total_pop,
Access = normalized.score(pct_below_185)) # get the normalized pct_below_185
library(tidyverse)
library(tidycensus)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
library(tidycensus)
#---Static Data---#
# PGH Tracts
tracts = read_csv('/Users/mattlampl/Desktop/Food Policy/HFPA_app/Data/Tract.csv') # Tracts from HFPA Estimate Sheet
pgh.tracts = as.character(tracts$Tract) # convert to vector
# no walkshed
no.walkshed = read_csv('/Users/mattlampl/Desktop/Food Policy/HFPA_app/Data/no_walkshed.csv')
no.walkshed = mutate(no.walkshed, GEOID = as.character(GEOID))
# Census API Key
census_api_key("842ebd1a7a562d5aa98015a367bdb6c7c4cc5a9b")
# get.census.var funcion
get.census.var = function(var.id, year = 2020, var.name, geometry = FALSE) {
var.df = get_acs(
geography = 'tract',
variables = var.id, # This is the var.id from https://api.census.gov/data/2020/acs/acs5/variables.html
year = year, # Supplied Year
state = 'PA',
county = 'Allegheny',
geometry = geometry
)
var.df = var.df %>%
select(GEOID, estimate) %>%
filter(GEOID %in% pgh.tracts) %>%
rename(!!var.name := estimate) %>%
mutate(year = year) # Add column for year
return(var.df)
}
# Normalized Score Function
normalized.score = function(unstandardized) {
standardized = (unstandardized - mean(unstandardized, na.rm = T)) / sd(unstandardized, na.rm = T)
normalized = (standardized - min(standardized, na.rm = T)) * (10 / (max(standardized, na.rm = T) - min(standardized, na.rm = T)))
return(normalized)
}
# Utilization Metrics Function
# 500 Cities: Census Tract-level Data
# Create a way to get the closest year's data for example passing 2020 would get 2019 data, passing in 2015 would get 2016 data
# This is a hardcoded dataframe. To find new URL endpoints for different years go here:
# https://chronicdata.cdc.gov/browse?category=500+Cities+%26+Places&q=500+Cities%3A+Census+Tract-level+Data&sortBy=relevance
file.names = c('k86t-wghb.csv', 'k25u-mg9b.csv', 'kucs-wizg.csv', '5mtz-k78d.csv')
years = c(2019, 2018, 2017, 2016)
API.endpoints = data.frame(year = years, file.name = file.names)
get.500.cities.endpoint = function(supplied.year) {
closest.year = which(abs(API.endpoints$year - supplied.year) == min(abs(API.endpoints$year - supplied.year))) # Closest year
endpoint = API.endpoints[closest.year, 'file.name']
return(endpoint)
}
# Get current year and lat 5 and 10 years
current.year = as.numeric(format(Sys.Date(), '%Y'))
last.10.years = seq(from = current.year - 2, to = current.year - 11, by = -1)
last.5.years = seq(from = current.year - 2, to = current.year - 6, by = -1)
# Empty HFPA.data data frame
HFPA.data = data.frame()
#-----------------#
for (year in last.5.years) {
# Start with availability
year.data = get.census.var(var.id = 'B08201_001', year = year, var.name = 'Total_HH', geometry = TRUE) # total_HH
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'B08014_002', year = year, var.name = 'No_Vehicle_Avail'),
c(GEOID, No_Vehicle_Avail)), by = 'GEOID') # Join in No_Vehicle_Avail
year.data = year.data %>%
mutate(pct_no_veh_avail = Total_HH/No_Vehicle_Avail,
pct_no_veh_avail = replace(pct_no_veh_avail, is.nan(pct_no_veh_avail) | is.infinite(pct_no_veh_avail), NA), # Replace NaN and Inf with NA
No_Vehicle_Score = normalized.score(pct_no_veh_avail)) # get no_vehicle_score
year.data = year.data %>%
inner_join(y = no.walkshed, by = 'GEOID') %>% # Join no_walkshed
mutate(Walkability_Score = normalized.score(no_walkshed)) # Get walkability_score
year.data = year.data %>%
mutate(Availability = sqrt(No_Vehicle_Score * Walkability_Score)) # get final Availiability score
# Join in the Access metrics
# Get pop below 185% Poverty Level
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'S1701_C01_041', year = year, var.name = 'pop_below_185'), c(GEOID, pop_below_185)), by = 'GEOID') %>%
inner_join(y = select(get.census.var(var.id = 'S0101_C01_001', year = year, var.name = 'total_pop'), c(GEOID, total_pop)), by = 'GEOID')
# Create the rest of the columns
year.data = year.data %>%
mutate(pct_below_185 = pop_below_185 / total_pop,
Access = normalized.score(pct_below_185)) # get the normalized pct_below_185
# Join in the Utilization metrics
# create the url from the get.500.cities.endpoint function
URL = paste('https://chronicdata.cdc.gov/resource/', get.500.cities.endpoint(year), '?placename=Pittsburgh', sep='')
utilization = read_csv(URL) # create temp dataframe with just that year's data
# create necessary normalized columns for the year
utilization = utilization %>%
mutate(GEOID = as.character(tractfips)) %>%  # Change to GEOID
select(GEOID, obesity_crudeprev, chd_crudeprev, diabetes_crudeprev) %>% # Keep only necessary columns
mutate(obesity_norm = normalized.score(obesity_crudeprev), # Normalize the columns
chd_norm = normalized.score(chd_crudeprev),
diabetes_norm = normalized.score(diabetes_crudeprev),
Utilization = obesity_norm + chd_norm + diabetes_norm) # Calculate utilization!
# Join it in with the rest
year.data = year.data %>%
left_join(y = utilization, by = 'GEOID')
# Bind everything above to the main HFPA.data dataframe and make final HFPA column
HFPA.data = rbind(HFPA.data, year.data)
}
HFPA.data = HFPA.data %>%
mutate(HFPA = Availability + Access + Utilization)
source("~/Desktop/Food Policy/HFPA_app/Heatlhy-Food-Priority-Areas-Dashboard/get_data.R", echo=TRUE)
library(tidyverse)
library(tidycensus)
#---Static Data---#
# PGH Tracts
tracts = read_csv('Data/Tract.csv') # Tracts from HFPA Estimate Sheet
pgh.tracts = as.character(tracts$Tract) # convert to vector
# no walkshed
no.walkshed = read_csv('Data/no_walkshed.csv')
no.walkshed = mutate(no.walkshed, GEOID = as.character(GEOID))
# Census API Key
census_api_key("842ebd1a7a562d5aa98015a367bdb6c7c4cc5a9b")
# get.census.var funcion
get.census.var = function(var.id, year = 2020, var.name, geometry = FALSE) {
var.df = get_acs(
geography = 'tract',
variables = var.id, # This is the var.id from https://api.census.gov/data/2020/acs/acs5/variables.html
year = year, # Supplied Year
state = 'PA',
county = 'Allegheny',
geometry = geometry
)
var.df = var.df %>%
select(GEOID, estimate) %>%
filter(GEOID %in% pgh.tracts) %>%
rename(!!var.name := estimate) %>%
mutate(year = year) # Add column for year
return(var.df)
}
# Normalized Score Function
normalized.score = function(unstandardized) {
standardized = (unstandardized - mean(unstandardized, na.rm = T)) / sd(unstandardized, na.rm = T)
normalized = (standardized - min(standardized, na.rm = T)) * (10 / (max(standardized, na.rm = T) - min(standardized, na.rm = T)))
return(normalized)
}
# Utilization Metrics Function
# 500 Cities: Census Tract-level Data
# Create a way to get the closest year's data for example passing 2020 would get 2019 data, passing in 2015 would get 2016 data
# This is a hardcoded dataframe. To find new URL endpoints for different years go here:
# https://chronicdata.cdc.gov/browse?category=500+Cities+%26+Places&q=500+Cities%3A+Census+Tract-level+Data&sortBy=relevance
file.names = c('k86t-wghb.csv', 'k25u-mg9b.csv', 'kucs-wizg.csv', '5mtz-k78d.csv')
years = c(2019, 2018, 2017, 2016)
API.endpoints = data.frame(year = years, file.name = file.names)
get.500.cities.endpoint = function(supplied.year) {
closest.year = which(abs(API.endpoints$year - supplied.year) == min(abs(API.endpoints$year - supplied.year))) # Closest year
endpoint = API.endpoints[closest.year, 'file.name']
return(endpoint)
}
# Get current year and lat 5 and 10 years
current.year = as.numeric(format(Sys.Date(), '%Y'))
last.10.years = seq(from = current.year - 2, to = current.year - 10, by = -1)
last.5.years = seq(from = current.year - 2, to = current.year - 6, by = -1)
get.data = function() {
# Empty HFPA.data data frame
HFPA.data = data.frame()
#-----------------#
for (year in last.10.years) {
# Start with availability
year.data = get.census.var(var.id = 'B08201_001', year = year, var.name = 'Total_HH', geometry = TRUE) # total_HH
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'B08014_002', year = year, var.name = 'No_Vehicle_Avail'),
c(GEOID, No_Vehicle_Avail)), by = 'GEOID') # Join in No_Vehicle_Avail
year.data = year.data %>%
mutate(pct_no_veh_avail = Total_HH/No_Vehicle_Avail,
pct_no_veh_avail = replace(pct_no_veh_avail, is.nan(pct_no_veh_avail) | is.infinite(pct_no_veh_avail), NA), # Replace NaN and Inf with NA
No_Vehicle_Score = normalized.score(pct_no_veh_avail)) # get no_vehicle_score
year.data = year.data %>%
inner_join(y = no.walkshed, by = 'GEOID') %>% # Join no_walkshed
mutate(Walkability_Score = normalized.score(no_walkshed)) # Get walkability_score
year.data = year.data %>%
mutate(Availability = sqrt(No_Vehicle_Score * Walkability_Score)) # get final Availiability score
# Join in the Access metrics
# Get pop below 185% Poverty Level
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'S1701_C01_041', year = year, var.name = 'pop_below_185'), c(GEOID, pop_below_185)), by = 'GEOID') %>%
inner_join(y = select(get.census.var(var.id = 'S0101_C01_001', year = year, var.name = 'total_pop'), c(GEOID, total_pop)), by = 'GEOID')
# Create the rest of the columns
year.data = year.data %>%
mutate(pct_below_185 = pop_below_185 / total_pop,
Access = normalized.score(pct_below_185)) # get the normalized pct_below_185
# Join in the Utilization metrics
# create the url from the get.500.cities.endpoint function
URL = paste('https://chronicdata.cdc.gov/resource/', get.500.cities.endpoint(year), '?placename=Pittsburgh', sep='')
utilization = read_csv(URL) # create temp dataframe with just that year's data
# create necessary normalized columns for the year
utilization = utilization %>%
mutate(GEOID = as.character(tractfips)) %>%  # Change to GEOID
select(GEOID, obesity_crudeprev, chd_crudeprev, diabetes_crudeprev) %>% # Keep only necessary columns
mutate(obesity_norm = normalized.score(obesity_crudeprev), # Normalize the columns
chd_norm = normalized.score(chd_crudeprev),
diabetes_norm = normalized.score(diabetes_crudeprev),
Utilization = obesity_norm + chd_norm + diabetes_norm,
Utilization = normalized.score(Utilization)) # Calculate utilization!
# Join it in with the rest
year.data = year.data %>%
left_join(y = utilization, by = 'GEOID')
# Bind everything above to the main HFPA.data dataframe and make final HFPA column
HFPA.data = rbind(HFPA.data, year.data)
}
HFPA.data = HFPA.data %>%
mutate(HFPA = Availability + Access + Utilization) %>%
group_by(year) %>%
mutate(Priority.Area = case_when(HFPA >= (mean(HFPA, na.rm = T) + sd(HFPA, na.rm = T)) ~ '1 SD Above Mean',
HFPA <= (mean(HFPA, na.rm = T) - sd(HFPA, na.rm = T)) ~ '1 SD Below Mean',
TRUE ~ 'Not Anomalous'
)) %>%
ungroup() %>%
mutate(Priority.Area = ordered(x = Priority.Area, levels = c('1 SD Above Mean', 'Not Anomalous', '1 SD Below Mean')))
return (HFPA.data)
}
HFPA.data = get.data()
setwd("~/Desktop/Food Policy/HFPA_app/Heatlhy-Food-Priority-Areas-Dashboard")
library(tidyverse)
library(tidycensus)
#---Static Data---#
# PGH Tracts
tracts = read_csv('Data/Tract.csv') # Tracts from HFPA Estimate Sheet
pgh.tracts = as.character(tracts$Tract) # convert to vector
# no walkshed
no.walkshed = read_csv('Data/no_walkshed.csv')
no.walkshed = mutate(no.walkshed, GEOID = as.character(GEOID))
# Census API Key
census_api_key("842ebd1a7a562d5aa98015a367bdb6c7c4cc5a9b")
# get.census.var funcion
get.census.var = function(var.id, year = 2020, var.name, geometry = FALSE) {
var.df = get_acs(
geography = 'tract',
variables = var.id, # This is the var.id from https://api.census.gov/data/2020/acs/acs5/variables.html
year = year, # Supplied Year
state = 'PA',
county = 'Allegheny',
geometry = geometry
)
var.df = var.df %>%
select(GEOID, estimate) %>%
filter(GEOID %in% pgh.tracts) %>%
rename(!!var.name := estimate) %>%
mutate(year = year) # Add column for year
return(var.df)
}
# Normalized Score Function
normalized.score = function(unstandardized) {
standardized = (unstandardized - mean(unstandardized, na.rm = T)) / sd(unstandardized, na.rm = T)
normalized = (standardized - min(standardized, na.rm = T)) * (10 / (max(standardized, na.rm = T) - min(standardized, na.rm = T)))
return(normalized)
}
# Utilization Metrics Function
# 500 Cities: Census Tract-level Data
# Create a way to get the closest year's data for example passing 2020 would get 2019 data, passing in 2015 would get 2016 data
# This is a hardcoded dataframe. To find new URL endpoints for different years go here:
# https://chronicdata.cdc.gov/browse?category=500+Cities+%26+Places&q=500+Cities%3A+Census+Tract-level+Data&sortBy=relevance
file.names = c('k86t-wghb.csv', 'k25u-mg9b.csv', 'kucs-wizg.csv', '5mtz-k78d.csv')
years = c(2019, 2018, 2017, 2016)
API.endpoints = data.frame(year = years, file.name = file.names)
get.500.cities.endpoint = function(supplied.year) {
closest.year = which(abs(API.endpoints$year - supplied.year) == min(abs(API.endpoints$year - supplied.year))) # Closest year
endpoint = API.endpoints[closest.year, 'file.name']
return(endpoint)
}
# Get current year and lat 5 and 10 years
current.year = as.numeric(format(Sys.Date(), '%Y'))
last.10.years = seq(from = current.year - 2, to = current.year - 10, by = -1)
last.5.years = seq(from = current.year - 2, to = current.year - 6, by = -1)
get.data = function() {
# Empty HFPA.data data frame
HFPA.data = data.frame()
#-----------------#
for (year in last.10.years) {
# Start with availability
year.data = get.census.var(var.id = 'B08201_001', year = year, var.name = 'Total_HH', geometry = TRUE) # total_HH
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'B08014_002', year = year, var.name = 'No_Vehicle_Avail'),
c(GEOID, No_Vehicle_Avail)), by = 'GEOID') # Join in No_Vehicle_Avail
year.data = year.data %>%
mutate(pct_no_veh_avail = Total_HH/No_Vehicle_Avail,
pct_no_veh_avail = replace(pct_no_veh_avail, is.nan(pct_no_veh_avail) | is.infinite(pct_no_veh_avail), NA), # Replace NaN and Inf with NA
No_Vehicle_Score = normalized.score(pct_no_veh_avail)) # get no_vehicle_score
year.data = year.data %>%
inner_join(y = no.walkshed, by = 'GEOID') %>% # Join no_walkshed
mutate(Walkability_Score = normalized.score(no_walkshed)) # Get walkability_score
year.data = year.data %>%
mutate(Availability = sqrt(No_Vehicle_Score * Walkability_Score)) # get final Availiability score
# Join in the Access metrics
# Get pop below 185% Poverty Level
year.data = year.data %>%
inner_join(y = select(get.census.var(var.id = 'S1701_C01_041', year = year, var.name = 'pop_below_185'), c(GEOID, pop_below_185)), by = 'GEOID') %>%
inner_join(y = select(get.census.var(var.id = 'S0101_C01_001', year = year, var.name = 'total_pop'), c(GEOID, total_pop)), by = 'GEOID')
# Create the rest of the columns
year.data = year.data %>%
mutate(pct_below_185 = pop_below_185 / total_pop,
Access = normalized.score(pct_below_185)) # get the normalized pct_below_185
# Join in the Utilization metrics
# create the url from the get.500.cities.endpoint function
URL = paste('https://chronicdata.cdc.gov/resource/', get.500.cities.endpoint(year), '?placename=Pittsburgh', sep='')
utilization = read_csv(URL) # create temp dataframe with just that year's data
# create necessary normalized columns for the year
utilization = utilization %>%
mutate(GEOID = as.character(tractfips)) %>%  # Change to GEOID
select(GEOID, obesity_crudeprev, chd_crudeprev, diabetes_crudeprev) %>% # Keep only necessary columns
mutate(obesity_norm = normalized.score(obesity_crudeprev), # Normalize the columns
chd_norm = normalized.score(chd_crudeprev),
diabetes_norm = normalized.score(diabetes_crudeprev),
Utilization = obesity_norm + chd_norm + diabetes_norm,
Utilization = normalized.score(Utilization)) # Calculate utilization!
# Join it in with the rest
year.data = year.data %>%
left_join(y = utilization, by = 'GEOID')
# Bind everything above to the main HFPA.data dataframe and make final HFPA column
HFPA.data = rbind(HFPA.data, year.data)
}
HFPA.data = HFPA.data %>%
mutate(HFPA = Availability + Access + Utilization) %>%
group_by(year) %>%
mutate(Priority.Area = case_when(HFPA >= (mean(HFPA, na.rm = T) + sd(HFPA, na.rm = T)) ~ '1 SD Above Mean',
HFPA <= (mean(HFPA, na.rm = T) - sd(HFPA, na.rm = T)) ~ '1 SD Below Mean',
TRUE ~ 'Not Anomalous'
)) %>%
ungroup() %>%
mutate(Priority.Area = ordered(x = Priority.Area, levels = c('1 SD Above Mean', 'Not Anomalous', '1 SD Below Mean')))
return (HFPA.data)
}
HFPA.data = get.data()
write_csv(HFPA.data, 'HFPA_data.csv')
library(tidyverse)
library(ggthemes)
library(class)
?case_when
HFPA.knn = HFPA.data %>%
mutate(Priority.Class =
case_when(Priority.Area == '1 SD Above Mean' ~ 1,
T ~ 0),
Priority.Class = as.factor(Priority.Class))
View(HFPA.knn)
library(tidycensus)
HFPA.knn = HFPA.data %>%
mutate(Priority.Class =
case_when(Priority.Area == '1 SD Above Mean' ~ 1,
T ~ 0),
Priority.Class = as.factor(Priority.Class)) %>%
st_set_geometry(NULL)
library(sf)
HFPA.knn = HFPA.data %>%
mutate(Priority.Class =
case_when(Priority.Area == '1 SD Above Mean' ~ 1,
T ~ 0),
Priority.Class = as.factor(Priority.Class)) %>%
st_set_geometry(NULL)
write_csv(HFPA.knn, 'HFPA_class.csv')
